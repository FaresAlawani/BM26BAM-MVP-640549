{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Xeno Underwriting Services: modelling and process explanation \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook will go through the modelling process that Xeno Underwriting services adapts in its products. The code and the basic structure of the following notebook is taken from the research paper listed below, with the relevent changes being explained. The variables used in this model can be verified from the document in the second source listed below.\n",
        "\n",
        "[1]\n",
        "A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor Third-Party Liability Claims (November 8, 2018). doi:10.2139/ssrn.3164764\n",
        "\n",
        "[2]\n",
        "\"CASdatasets\" package supporting document. http://cas.uqam.ca/pub/web/CASdatasets-manual.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Tweedie regression on insurance claims\n",
        "\n",
        "This example illustrates the use of Tweedie regression on\n",
        "the [French Motor Third-Party Liability Claims dataset](https://www.openml.org/d/41214), and is inspired by an R tutorial [1]_.\n",
        "\n",
        "In this dataset, each sample corresponds to an insurance policy, i.e. a\n",
        "contract within an insurance company and an individual (policyholder).\n",
        "Available features include driver age, vehicle age, vehicle power, etc.\n",
        "\n",
        "A few definitions: a *claim* is the request made by a policyholder to the\n",
        "insurer to compensate for a loss covered by the insurance. The *claim amount*\n",
        "is the amount of money that the insurer must pay. The *exposure* is the\n",
        "duration of the insurance coverage of a given policy, in years.\n",
        "\n",
        "Here our goal is to predict the expected\n",
        "value, i.e. the mean, of the total claim amount per exposure unit also\n",
        "referred to as the pure premium.\n",
        "\n",
        "There are several possibilities to do that, two of which are:\n",
        "\n",
        "1. Model the number of claims with a Poisson distribution, and the average\n",
        "   claim amount per claim, also known as severity, as a Gamma distribution\n",
        "   and multiply the predictions of both in order to get the total claim\n",
        "   amount.\n",
        "2. Model the total claim amount per exposure directly, typically with a Tweedie\n",
        "   distribution of Tweedie power $p \\in (1, 2)$.\n",
        "\n",
        "In this example we will illustrate both approaches. We start by defining a few\n",
        "helper functions for loading the data and visualizing results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Christian Lorentzen <lorentzen.ch@gmail.com>\n",
        "#          Roman Yurchak <rth.yurchak@gmail.com>\n",
        "#          Olivier Grisel <olivier.grisel@ensta.org>\n",
        "# License: BSD 3 clause"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import mean_tweedie_deviance\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "def load_mtpl2(n_samples=None):\n",
        "    \"\"\"Fetch the French Motor Third-Party Liability Claims dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples: int, default=None\n",
        "      number of samples to select (for faster run time). Full dataset has\n",
        "      678013 samples.\n",
        "    \"\"\"\n",
        "    # freMTPL2freq dataset from https://www.openml.org/d/41214\n",
        "    df_freq = fetch_openml(data_id=41214, as_frame=True, parser=\"pandas\").data\n",
        "    df_freq[\"IDpol\"] = df_freq[\"IDpol\"].astype(int)\n",
        "    df_freq.set_index(\"IDpol\", inplace=True)\n",
        "\n",
        "    # freMTPL2sev dataset from https://www.openml.org/d/41215\n",
        "    df_sev = fetch_openml(data_id=41215, as_frame=True, parser=\"pandas\").data\n",
        "\n",
        "    # sum ClaimAmount over identical IDs\n",
        "    df_sev = df_sev.groupby(\"IDpol\").sum()\n",
        "\n",
        "    df = df_freq.join(df_sev, how=\"left\")\n",
        "    df[\"ClaimAmount\"].fillna(0, inplace=True)\n",
        "\n",
        "    # unquote string fields\n",
        "    for column_name in df.columns[df.dtypes.values == object]:\n",
        "        df[column_name] = df[column_name].str.strip(\"'\")\n",
        "    return df.iloc[:n_samples]\n",
        "\n",
        "\n",
        "def plot_obs_pred(\n",
        "    df,\n",
        "    feature,\n",
        "    weight,\n",
        "    observed,\n",
        "    predicted,\n",
        "    y_label=None,\n",
        "    title=None,\n",
        "    ax=None,\n",
        "    fill_legend=False,\n",
        "):\n",
        "    \"\"\"Plot observed and predicted - aggregated per feature level.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        input data\n",
        "    feature: str\n",
        "        a column name of df for the feature to be plotted\n",
        "    weight : str\n",
        "        column name of df with the values of weights or exposure\n",
        "    observed : str\n",
        "        a column name of df with the observed target\n",
        "    predicted : DataFrame\n",
        "        a dataframe, with the same index as df, with the predicted target\n",
        "    fill_legend : bool, default=False\n",
        "        whether to show fill_between legend\n",
        "    \"\"\"\n",
        "    # aggregate observed and predicted variables by feature level\n",
        "    df_ = df.loc[:, [feature, weight]].copy()\n",
        "    df_[\"observed\"] = df[observed] * df[weight]\n",
        "    df_[\"predicted\"] = predicted * df[weight]\n",
        "    df_ = (\n",
        "        df_.groupby([feature])[[weight, \"observed\", \"predicted\"]]\n",
        "        .sum()\n",
        "        .assign(observed=lambda x: x[\"observed\"] / x[weight])\n",
        "        .assign(predicted=lambda x: x[\"predicted\"] / x[weight])\n",
        "    )\n",
        "\n",
        "    ax = df_.loc[:, [\"observed\", \"predicted\"]].plot(style=\".\", ax=ax)\n",
        "    y_max = df_.loc[:, [\"observed\", \"predicted\"]].values.max() * 0.8\n",
        "    p2 = ax.fill_between(\n",
        "        df_.index,\n",
        "        0,\n",
        "        y_max * df_[weight] / df_[weight].values.max(),\n",
        "        color=\"g\",\n",
        "        alpha=0.1,\n",
        "    )\n",
        "    if fill_legend:\n",
        "        ax.legend([p2], [\"{} distribution\".format(feature)])\n",
        "    ax.set(\n",
        "        ylabel=y_label if y_label is not None else None,\n",
        "        title=title if title is not None else \"Train: Observed vs Predicted\",\n",
        "    )\n",
        "\n",
        "\n",
        "def score_estimator(\n",
        "    estimator,\n",
        "    X_train,\n",
        "    X_test,\n",
        "    df_train,\n",
        "    df_test,\n",
        "    target,\n",
        "    weights,\n",
        "    tweedie_powers=None,\n",
        "):\n",
        "    \"\"\"Evaluate an estimator on train and test sets with different metrics\"\"\"\n",
        "\n",
        "    metrics = [\n",
        "        (\"D² explained\", None),  # Use default scorer if it exists\n",
        "        (\"mean abs. error\", mean_absolute_error),\n",
        "        (\"mean squared error\", mean_squared_error),\n",
        "    ]\n",
        "    if tweedie_powers:\n",
        "        metrics += [\n",
        "            (\n",
        "                \"mean Tweedie dev p={:.4f}\".format(power),\n",
        "                partial(mean_tweedie_deviance, power=power),\n",
        "            )\n",
        "            for power in tweedie_powers\n",
        "        ]\n",
        "\n",
        "    res = []\n",
        "    for subset_label, X, df in [\n",
        "        (\"train\", X_train, df_train),\n",
        "        (\"test\", X_test, df_test),\n",
        "    ]:\n",
        "        y, _weights = df[target], df[weights]\n",
        "        for score_label, metric in metrics:\n",
        "            if isinstance(estimator, tuple) and len(estimator) == 2:\n",
        "                # Score the model consisting of the product of frequency and\n",
        "                # severity models.\n",
        "                est_freq, est_sev = estimator\n",
        "                y_pred = est_freq.predict(X) * est_sev.predict(X)\n",
        "            else:\n",
        "                y_pred = estimator.predict(X)\n",
        "\n",
        "            if metric is None:\n",
        "                if not hasattr(estimator, \"score\"):\n",
        "                    continue\n",
        "                score = estimator.score(X, y, sample_weight=_weights)\n",
        "            else:\n",
        "                score = metric(y, y_pred, sample_weight=_weights)\n",
        "\n",
        "            res.append({\"subset\": subset_label, \"metric\": score_label, \"score\": score})\n",
        "\n",
        "    res = (\n",
        "        pd.DataFrame(res)\n",
        "        .set_index([\"metric\", \"subset\"])\n",
        "        .score.unstack(-1)\n",
        "        .round(4)\n",
        "        .loc[:, [\"train\", \"test\"]]\n",
        "    )\n",
        "    return res"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading datasets, basic feature extraction and target definitions\n",
        "\n",
        "We construct the freMTPL2 dataset by joining the freMTPL2freq table,\n",
        "containing the number of claims (``ClaimNb``), with the freMTPL2sev table,\n",
        "containing the claim amount (``ClaimAmount``) for the same policy ids\n",
        "(``IDpol``).\n",
        "\n",
        "#### Filtering and Correcting Data:\n",
        "\n",
        "The code filters out claims with zero amounts and sets their corresponding claim count (ClaimNb) to zero. This is done to ensure that the severity model, which requires strictly positive target values, can be applied.\n",
        "\n",
        "Unreasonable observations are corrected to mitigate potential data errors. Specifically, ClaimNb, Exposure, and ClaimAmount are clipped to upper limits of 4, 1, and 200,000, respectively. This prevents exceptionally large claim amounts and exposure values.\n",
        "\n",
        "\n",
        "#### Data Transformation:\n",
        "\n",
        "ColumnTransformer is then used to define the following set of trasnformers:\n",
        "\n",
        "- KBinsDiscretizer is used to discretize the \"VehAge\" and \"DrivAge\" features into 10 bins each, resulting in binned numeric features.\n",
        "\n",
        "- OneHotEncoder is applied to \"VehBrand\", \"VehPower\", \"VehGas\",  to convert categorical variables into one-hot encoded features.\n",
        "\n",
        "\n",
        "- \"Density\" is log-scaled using FunctionTransformer with a logarithm function.\n",
        "\n",
        "#### Feature Extraction\n",
        "\n",
        "The \"PurePremium\" feature represents the expected total claim amount per unit of exposure for each policyholder in the insurance company's portfolio. It is computed by dividing the \"ClaimAmount\" by the \"Exposure\" in the dataset.\n",
        "\n",
        "#### Dropping Redundant Variables:\n",
        "\n",
        "The \"Area\", \"Region\", and \"BonusMalus\" columns are dropped from the training data as they are only concerned with French areas and insurance rating, while this MVP is directed towards clients in different parts of Europe. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = load_mtpl2()\n",
        "\n",
        "# Correct for unreasonable observations (that might be data error)\n",
        "# and a few exceptionally large claim amounts\n",
        "df[\"ClaimNb\"] = df[\"ClaimNb\"].clip(upper=4)\n",
        "df[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)\n",
        "df[\"ClaimAmount\"] = df[\"ClaimAmount\"].clip(upper=200000)\n",
        "\n",
        "log_scale_transformer = make_pipeline(FunctionTransformer(func=np.log), StandardScaler())\n",
        "\n",
        "column_trans = ColumnTransformer(\n",
        "    [(\"binned_numeric\",KBinsDiscretizer(n_bins=10, subsample=int(2e5), random_state=0),[\"VehAge\", \"DrivAge\"],),\n",
        "    (\"onehot_categorical\",OneHotEncoder(), [\"VehBrand\", \"VehPower\", \"VehGas\"],),\n",
        "    (\"log_scaled_numeric\", log_scale_transformer, [\"Density\"]),], remainder=\"drop\",)\n",
        "\n",
        "X = column_trans.fit_transform(df)\n",
        "\n",
        "# Pure Premium \n",
        "df[\"PurePremium\"] = df[\"ClaimAmount\"] / df[\"Exposure\"]\n",
        "\n",
        "\n",
        "#### dropping redundant variables for Xeno's applications\n",
        "df = df.drop(['Area', 'Region', 'BonusMalus'], axis=1)\n",
        "### \n",
        "\n",
        "##Splitting the dataframe to train and test data \n",
        "df_train, df_test, X_train, X_test = train_test_split(df, X, random_state=0)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "####\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables \n",
        "\n",
        "PurePremium: the expected total claim amount per unit of exposure for each policyholder in their portfolio. \n",
        "\n",
        "ClaimNb: The number of claims.\n",
        "\n",
        "Exposure: The number of policy years.\n",
        "\n",
        "VehPower: The vehicle power (as \"factor\") from least powerful \"P2\" to most powerful car \"P15\".\n",
        "\n",
        "VehAge: The vehicle age, in years.\n",
        "\n",
        "DrivAge: The age of the policyholder.\n",
        "\n",
        "VehBrand: The car brand divided in the following groups: A- Renaut Nissan and Citroen, B- Volkswagen, Audi, Skoda and Seat, C- Opel, General Motors and Ford, D- Fiat, E- Mercedes Chrysler\n",
        "and BMW, F- Japanese (except Nissan) and Korean, G- other.\n",
        "\n",
        "VehGas: The car gas, Diesel or regular (as \"factor\").\n",
        "\n",
        "Density: The density of inhabitants (number of inhabitants per km2) in the city the driver of the\n",
        "car lives in.\n",
        "\n",
        "ClaimAmount: Total previous claim amount of the guarantee.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Pure Premium Modeling via a  TweedieRegressor\n",
        "We can model the total loss with a unique\n",
        "Compound Poisson Gamma generalized linear model (with a log link function).\n",
        "This model is a special case of the Tweedie GLM with a \"power\" parameter\n",
        "$p \\in (1, 2)$. Here, we fix apriori the `power` parameter of the\n",
        "Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one\n",
        "would select this value via grid-search by minimizing the negative\n",
        "log-likelihood of the Tweedie model, but unfortunately the current\n",
        "implementation does not allow for this (yet).\n",
        "\n",
        "We will compare the performance of both approaches.\n",
        "To quantify the performance of both models, one can compute\n",
        "the mean deviance of the train and test data assuming a Compound\n",
        "Poisson-Gamma distribution of the total claim amount. This is equivalent to\n",
        "a Tweedie distribution with a `power` parameter between 1 and 2.\n",
        "\n",
        "The :func:`sklearn.metrics.mean_tweedie_deviance` depends on a `power`\n",
        "parameter. As we do not know the true value of the `power` parameter, we here\n",
        "compute the mean deviances for a grid of possible values, and compare the\n",
        "models side by side, i.e. we compare them at identical values of `power`.\n",
        "Ideally, we hope that one model will be consistently better than the other,\n",
        "regardless of `power`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\n",
            "                          Product Model              \n",
            "subset                            train          test\n",
            "metric                                               \n",
            "D² explained               1.690000e-02  1.420000e-02\n",
            "mean Tweedie dev p=1.5000  7.640770e+01  7.640880e+01\n",
            "mean Tweedie dev p=1.7000  3.682880e+01  3.692270e+01\n",
            "mean Tweedie dev p=1.8000  3.037600e+01  3.045390e+01\n",
            "mean Tweedie dev p=1.9000  3.382120e+01  3.387830e+01\n",
            "mean Tweedie dev p=1.9900  2.015347e+02  2.015587e+02\n",
            "mean Tweedie dev p=1.9990  1.914538e+03  1.914387e+03\n",
            "mean Tweedie dev p=1.9999  1.904747e+04  1.904558e+04\n",
            "mean abs. error            2.739865e+02  2.731249e+02\n",
            "mean squared error         3.295505e+07  3.213056e+07\n",
            "[ 87.1174128   87.93388079  75.93685879 ... 117.15437623  74.95634597\n",
            "  70.43728507]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import TweedieRegressor\n",
        "from joblib import dump, load\n",
        "import pickle\n",
        "\n",
        "\n",
        "glm_pure_premium = TweedieRegressor(power=1.9, alpha=0.1, solver=\"newton-cholesky\")\n",
        "glm_pure_premium_1= glm_pure_premium.fit(\n",
        "    X_train, df_train[\"PurePremium\"], sample_weight=df_train[\"Exposure\"]\n",
        ")\n",
        "\n",
        "tweedie_powers = [1.5, 1.7, 1.8, 1.9, 1.99, 1.999, 1.9999]\n",
        "\n",
        "\n",
        "\n",
        "scores_glm_pure_premium = score_estimator(\n",
        "    glm_pure_premium,\n",
        "    X_train,\n",
        "    X_test,\n",
        "    df_train,\n",
        "    df_test,\n",
        "    target=\"PurePremium\",\n",
        "    weights=\"Exposure\",\n",
        "    tweedie_powers=tweedie_powers,)\n",
        "\n",
        "scores = pd.concat(\n",
        "    [scores_glm_pure_premium],\n",
        "    axis=1,\n",
        "    sort=True,\n",
        "    keys=(\"Product Model\", \"TweedieRegressor\"),)\n",
        "\n",
        "print(\"Evaluation of the Product Model and the Tweedie Regressor on target PurePremium\")\n",
        "with pd.option_context(\"display.expand_frame_repr\", False):\n",
        "    print(scores)\n",
        "\n",
        "#### exctracting the model \n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(glm_pure_premium_1, f)\n",
        "#### \n",
        "y_pred = glm_pure_premium_1.predict(X_test)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
